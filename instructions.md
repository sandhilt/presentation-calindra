# ğŸ¤– Tic Tac Toe com LLM Local (Qwen2.5-Coder via Ollama)

Este projeto cria um jogo da velha (tic tac toe) acessÃ­vel via navegador, onde qualquer pessoa pode jogar contra uma LLM local rodando via [Ollama](https://ollama.com/). A API Ã© feita em **Node.js com Express**, e o frontend Ã© baseado no tutorial oficial do React. Pode usar vite com typescript.

---

## ğŸ“¦ PrÃ©-requisitos

- Node.js >= 22
- Yarn ou npm
- Ollama instalado ([https://ollama.com](https://ollama.com))
- Ngrok instalado (`npm install -g ngrok`)

---

## ğŸ§± Estrutura do projeto
llm-tictactoe/
â”œâ”€â”€ backend/
â”‚ â””â”€â”€ server.js # API Express conectada Ã  Ollama
â”œâ”€â”€ frontend/
â”‚ â””â”€â”€ (React App) # UI baseada no tutorial oficial
â””â”€â”€ README.md


---

## ğŸ” Passo a passo de instalaÃ§Ã£o

### 1. Baixe o modelo LLM
Abra um terminal e rode:

```bash
ollama pull qwen2.5:coder
ollama run qwen2.5:coder
```

Deixe esse processo rodando.

### 2. Rode o backend Express

```bash
cd backend
npm install
node server.js
```

A API estarÃ¡ em http://localhost:3000/api/move

### 3. Instale e rode o frontend React

```bash
cd ../frontend
npm install
npm run dev
```

### 4. Exporte o frontend com ngrok

Em outro terminal:

```bash
ngrok http 5173
```

Copie o link HTTPS gerado, por exemplo: https://abcdef1234.ngrok.io

### 5. Gere o QR Code (opcional)
Instale e use:

```bash
npm install qrcode-terminal
npx qrcode-terminal https://abcdef1234.ngrok.io
```

Mostre esse QR Code na sua apresentaÃ§Ã£o para que qualquer pessoa com um celular acesse e jogue.

ğŸ§  Como a IA joga?
A cada jogada humana:
- O React envia o estado do tabuleiro para http://localhost:3000/api/move
- O Express chama o modelo LLM local via Ollama
- A LLM responde com uma posiÃ§Ã£o "linha,coluna" (ex: "1,2")
- O React atualiza o tabuleiro

ğŸ§ª Teste local
1. Abra o jogo no navegador via localhost ou link do ngrok
2. FaÃ§a uma jogada
3. Observe o terminal da API recebendo o tabuleiro
4. Veja a IA respondendo e jogando de volta

âœ¨ Objetivo
Essa demo mostra como:
- Usar uma LLM local como agente de decisÃ£o
- Conectar frontend, backend e inferÃªncia
- Compartilhar experiÃªncias com o pÃºblico sem login ou credenciais

ğŸ§° Extras
- Para logs, ative console.log(board) no server.js
- Para cache, vocÃª pode usar Redis ou salvar as partidas localmente
- Para multiplayer vs IA, Ã© possÃ­vel estender o backend para usar socket.io

ğŸ§¼ Para reiniciar
```bash
killall node  # (ou use Ctrl+C nos terminais)
```

Feito com â˜•, cÃ³digo e bilhÃµes de parÃ¢metros.